Best config yet:

Layers 48,32
One dropout inbetween 0.1/0.2
No batch normalization
Adam
kl_divergence
relu
learn 0.001

#Conv

64,64,64,64,64s Conv2d
Batch normalization (before activation)
Adam
categorical_crossentropy
relu
learn 0.001
No dropout
kernel_regularizer on every layer except last
Straight to softmax from conv2d (1,(1,1))