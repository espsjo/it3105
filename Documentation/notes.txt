Best config yet:

Layers 48,32
One dropout inbetween 0.1/0.2
No batch normalization
Adam
kl_divergence
relu
learn 0.001

#Conv

64,64,64 Conv2d
Batch normalization
Adam
kl_divergence
relu
learn 0.001
No dropout
Straight to softmax from conv2d (1,(1,1))